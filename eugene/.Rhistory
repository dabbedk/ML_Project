main = "Single K-Means Attempt", col = km.iris$cluster)
points(km.iris$centers[, 4], km.iris$centers[, 2], pch = 16, col = "blue")
#A function to help determine the number of clusters when we do not have an
#idea ahead of time.
wssplot = function(data, nc = 15, seed = 0) {
wss = (nrow(data) - 1) * sum(apply(data, 2, var))
for (i in 2:nc) {
set.seed(seed)
wss[i] = sum(kmeans(data, centers = i, iter.max = 100, nstart = 100)$withinss)
}
plot(1:nc, wss, type = "b",
xlab = "Number of Clusters",
ylab = "Within-Cluster Variance",
main = "Scree Plot for the K-Means Procedure")
}
#Visualizing the scree plot for the scaled iris data; 3 seems like a plausible
#choice.
wssplot(iris.scale)
View(iris.scale)
#Using the Old Faithful dataset.
faithful.scale = scale(faithful)
summary(faithful.scale)
#Visualizing the scaled data.
par(mfrow = c(1, 1))
plot(faithful.scale)
#Determining the number of clusters.
wssplot(faithful.scale)
View(iris.scale)
#Visualizing the scree plot for the scaled iris data; 3 seems like a plausible
#choice.
wssplot(iris.scale)
#Using the Old Faithful dataset.
faithful.scale = scale(faithful)
summary(faithful.scale)
#Visualizing the scaled data.
par(mfrow = c(1, 1))
plot(faithful.scale)
#Determining the number of clusters.
wssplot(faithful.scale)
#Clearly, by both visual inspection and an analysis of the scree plot, a 2
#cluster solution is the most appropriate; however, let's see what happens if
#we search for a 3 cluster solution.
set.seed(0)
km.faithful1 = kmeans(faithful.scale, centers = 3) #Running the K-means procedure
km.faithful2 = kmeans(faithful.scale, centers = 3) #5 different times, but with
km.faithful3 = kmeans(faithful.scale, centers = 3) #only one convergence of the
km.faithful4 = kmeans(faithful.scale, centers = 3) #algorithm each time.
km.faithful5 = kmeans(faithful.scale, centers = 3)
#Running the algorithm 100 different times and recording the solution with the
#lowest total within-cluster variance.
set.seed(0)
km.faithfulsim = kmeans(faithful.scale, centers = 3, nstart = 100)
#Visually & numerically inspecting the results.
par(mfrow = c(2, 3))
plot(faithful, col = km.faithful1$cluster,
main = paste("Single K-Means Attempt #1\n WCV: ",
round(km.faithful1$tot.withinss, 4)))
plot(faithful, col = km.faithful2$cluster,
main = paste("Single K-Means Attempt #2\n WCV: ",
round(km.faithful2$tot.withinss, 4)))
plot(faithful, col = km.faithful3$cluster,
main = paste("Single K-Means Attempt #3\n WCV: ",
round(km.faithful3$tot.withinss, 4)))
plot(faithful, col = km.faithful4$cluster,
main = paste("Single K-Means Attempt #4\n WCV: ",
round(km.faithful4$tot.withinss, 4)))
plot(faithful, col = km.faithful5$cluster,
main = paste("Single K-Means Attempt #5\n WCV: ",
round(km.faithful5$tot.withinss, 4)))
plot(faithful, col = km.faithfulsim$cluster,
main = paste("Best K-Means Attempt out of 100\n WCV: ",
round(km.faithfulsim$tot.withinss, 4)))
###########################################
#####Tools for Hierarchical Clustering#####
###########################################
library(flexclust) #Loading the flexclust library.
data(nutrient) #Loading the nutrient data.
help(nutrient) #Inspecting the data set; nutrients in meat, fish, and fowel.
nutrient
###########################################
#####Tools for Hierarchical Clustering#####
###########################################
library(flexclust) #Loading the flexclust library.
data(nutrient) #Loading the nutrient data.
help(nutrient) #Inspecting the data set; nutrients in meat, fish, and fowel.
View(nutrient)
#Notice that the nutrient columns are in different measurements: calories,
#grams, and milligrams.
summary(nutrient)
sapply(nutrient, sd)
#We should scale the data.
nutrient.scaled = as.data.frame(scale(nutrient))
summary(nutrient.scaled)
sapply(nutrient.scaled, sd)
#We need to calcualte the pairwise distances between observations.
d = dist(nutrient.scaled)
d
d
#Using the hclust() function, we define the linkage manner by which we will
#cluster our data.
fit.single = hclust(d, method = "single")
fit.complete = hclust(d, method = "complete")
fit.average = hclust(d, method = "average")
#Creating various dendrograms.
par(mfrow = c(1, 3))
plot(fit.single, hang = -1, main = "Dendrogram of Single Linkage")
plot(fit.complete, hang = -1, main = "Dendrogram of Complete Linkage")
plot(fit.average, hang = -1, main = "Dendrogram of Average Linkage")
#Cut the dendrogram into groups of data.
clusters.average = cutree(fit.average, k = 5)
clusters.average
#Viewing the groups of data.
table(clusters.average)
clusters.average
#Aggregating the original data by the cluster assignments.
aggregate(nutrient, by = list(cluster = clusters.average), median)
#Aggregating the scaled data by the cluster assignments.
aggregate(nutrient.scaled, by = list(cluster = clusters.average), median)
#Visualizing the groups in the dendrogram.
par(mfrow = c(1, 1))
plot(fit.average, hang = -1, main = "Dendrogram of Average Linkage\n5 Clusters")
rect.hclust(fit.average, k = 5)
#Viewing the groups of data.
table(clusters.average)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message=FALSE)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message=FALSE)
## Question #1: Orange Juice Data
**Purpose: Demonstrating Understanding of how to run the models**
1. **Load: ** Load the *OJ* dataset from the **ISLR** library into your workspace. The data contains 1,070 purchases where the customer either purchased Citrus Hill or Minute Maid orange juice. A number of characteristics of the customer and product are recorded.
```{r}
library(ISLR)
OJ
```
2. **Model a Tree: **Construct an initial decision tree predicting *Purchase* from all other variables in the training dataset.
+ Split the data into a training and test set with an 80% - 20% split, respectively.
+ Define splits based upon the Gini Coefficien
+ What are the training and testing accuracies?
```{r}
# train/test split
set.seed(0)
train = sample(1:nrow(OJ), 8*nrow(OJ)/10)
OJ.train = OJ[train, ]
OJ.test = OJ[-train, ]
# train/test split
set.seed(0)
train = sample(1:nrow(OJ), 8*nrow(OJ)/10)
OJ.train = OJ[train, ]
OJ.test = OJ[-train, ]
# construct initial tree
library(tree)
initial.tree = tree(Purchase ~ ., split = "gini", data = OJ.train)
# training accuracy
summary(initial.tree)
library(ISLR)
OJ
library(ISLR)
OJ
2. **Model a Tree: **Construct an initial decision tree predicting *Purchase* from all other variables in the training dataset.
+ Split the data into a training and test set with an 80% - 20% split, respectively.
+ Define splits based upon the Gini Coefficient
+ What are the training and testing accuracies?
```{r}
# train/test split
set.seed(0)
train = sample(1:nrow(OJ), 8*nrow(OJ)/10)
OJ.train = OJ[train, ]
OJ.test = OJ[-train, ]
# construct initial tree
library(tree)
initial.tree = tree(Purchase ~ ., split = "gini", data = OJ.train)
# training accuracy
summary(initial.tree)
# testing accuracy
tree.predictions = predict(initial.tree, OJ.test, type = "class")
table(tree.predictions, OJ.test$Purchase)
(106 + 59)/nrow(OJ.test)
#Approximately 77.10% of our predictions were correct on the test set using the
#initial tree.
3. **Cross_Validate: **Cross-Validate your tree using *prune.misclass* as your cost-complexity pruning function.
```{r}
set.seed(0)
cv.training = cv.tree(initial.tree, FUN = prune.misclass)
```
par(mfrow = c(1, 2))
plot(cv.training$size, cv.training$dev, type = "b",
xlab = "Terminal Nodes", ylab = "Misclassified Observations")
plot(cv.training$k, cv.training$dev, type  = "b",
xlab = "Alpha", ylab = "Misclassified Observations")
par(mfrow = c(1, 2))
plot(cv.training$size, cv.training$dev, type = "b",
xlab = "Terminal Nodes", ylab = "Misclassified Observations")
plot(cv.training$k, cv.training$dev, type  = "b",
xlab = "Alpha", ylab = "Misclassified Observations")
# optimal terminal nodes looks around 10, optimal alpha looks about 4
# prune initial tree with cv results
best.nodes = cv.training$size[which(cv.training$dev == min(cv.training$dev))] # optimal number of nodes is 11
best.nodes
# training accuracy
summary(prune.tree)  # the accuracy of the pruned tree is approximately 80%
# prune initial tree with cv results
best.nodes = cv.training$size[which(cv.training$dev == min(cv.training$dev))] # optimal number of nodes is 11
prune.tree = prune.misclass(initial.tree, best = best.nodes)
# training accuracy
summary(prune.tree)  # the accuracy of the pruned tree is approximately 80%
# testing accuracy
prune.predictions = predict(prune.tree, OJ.test, type = "class")
table(prune.predictions, OJ.test$Purchase)
(113 + 57)/nrow(OJ.test)
# visualize pruned tree
prune.tree
# visualize pruned tree
prune.tree
par(mfrow = c(1, 1))
plot(prune.tree)
text(prune.tree)
# visualize pruned tree
prune.tree
par(mfrow = c(1, 1))
plot(prune.tree)
text(prune.tree)
# prune initial tree with cv results
best.nodes = cv.training$size[which(cv.training$dev == min(cv.training$dev))] # optimal number of nodes is 11
prune.tree = prune.misclass(initial.tree, best = best.nodes)
# training accuracy
summary(prune.tree)  # the accuracy of the pruned tree is approximately 80%
# testing accuracy
prune.predictions = predict(prune.tree, OJ.test, type = "class")
table(prune.predictions, OJ.test$Purchase)
(113 + 57)/nrow(OJ.test)
# visualize pruned tree
prune.tree
par(mfrow = c(1, 1))
plot(prune.tree)
text(prune.tree)
# grow the forest
library(randomForest)
set.seed(0)
rf.default = randomForest(Purchase ~., data = OJ.train, importance = TRUE)
# training and testing accuracies
rf.default
table(predict(rf.default, OJ.test, type = "class"), OJ.test$Purchase)
(113 + 60)/nrow(OJ.test)
# top variable
importance(rf.default)
varImpPlot(rf.default)
set.seed(0)
oob.err = numeric(17)
for (mtry in 1:17) {
fit = randomForest(Purchase ~ ., data = OJ.train, mtry = mtry)
oob.err[mtry] = fit$err.rate[500, 1]
cat("We're performing iteration", mtry, "\n")
}
# visualize oob error rates
plot(1:17, oob.err, pch = 16, type = "b",
xlab = "Variables Considered at Each Split",
ylab = "OOB Misclassification Rate",
main = "Random Forest OOB Error Rates\nby # of Variables")
# maximum accuracy and optimal variables
1 - min(oob.err)
which(oob.err == min(oob.err))
# visualize oob error rates
plot(1:17, oob.err, pch = 16, type = "b",
xlab = "Variables Considered at Each Split",
ylab = "OOB Misclassification Rate",
main = "Random Forest OOB Error Rates\nby # of Variables")
# maximum accuracy and optimal variables
1 - min(oob.err)
which(oob.err == min(oob.err))
# bagged model accuracy
1 - oob.err[17]
set.seed(0)
rf.2var = randomForest(Purchase ~ ., data = OJ.train, mtry = 2)
table(predict(rf.2var, OJ.test, type = "class"), OJ.test$Purchase)
(118 + 58)/nrow(OJ.test)
set.seed(0)
rf.bagged = randomForest(Purchase ~ ., data = OJ.train, mtry = 17)
rf.bagged = randomForest(Purchase ~ ., data = OJ.train, mtry = 17)
table(predict(rf.bagged, OJ.test, type = "class"), OJ.test$Purchase)
table(predict(rf.bagged, OJ.test, type = "class"), OJ.test$Purchase)
(110 + 62)/nrow(OJ.test)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message=FALSE)
wine = read.csv("https://s3.amazonaws.com/nycdsabt01/Wine+Quality.csv")
2. Munge: Perform some data munging:
a. Recode the quality variable to be a factor variable with values of “Low” for       quality ratings of 5 and below, and “High” for ratings of 6 and above.
b. Scale and center the numeric vectors of your dataset.
```{r}
quality = ifelse(wine$quality <= 5, "Low", "High")
wine.scale = as.data.frame(scale(wine[, -12]))
wine = cbind(wine.scale, quality)
```
quality = ifelse(wine$quality <= 5, "Low", "High")
wine.scale = as.data.frame(scale(wine[, -12]))
wine = cbind(wine.scale, quality)
View(wine)
quality = ifelse(wine$quality <= 5, "Low", "High")
wine.scale = as.data.frame(scale(wine[, -12]))
wine = cbind(wine.scale, quality)
3. Split: Split the data into a training and test set with an 80% - 20% split, respectively. (NB: Use set.seed(0) so your results will be reproducible.)
```{r}
set.seed(0)
train = sample(1:nrow(wine), 8*nrow(wine)/10)
wine.train = wine[train, ]
wine.test = wine[-train, ]
4. Plot: Briefly explore some graphical EDA:
a. Explain why a maximal margin classifier is impossible to fit to this data. (NB:    Do not try to fit the maximal margin classifier.)
b. Explain why a support vector classifier is generally more desirable.
```{r}
plot(wine[, -12], col = wine$quality)
plot(wine[, -12], col = wine$quality)
View(wine)
plot(wine[, -12], col = wine$quality)
plot(wine[, -12], col = as.factor(wine$quality))
library(e1071)
set.seed(0)
cv.wine.svc.linear = tune(svm,
quality ~ .,
data = wine.train,
kernel = "linear",
ranges = list(cost = 10^(seq(-5, -.5, length = 50))))
cv.wine.svc.linear = tune(svm,
quality ~ .,
data = wine.train,
kernel = "linear",
ranges = list(cost = 10^(seq(-5, -.5, length = 50))))
library(e1071)
set.seed(0)
cv.wine.svc.linear = tune(svm,
quality ~ .,
data = wine.train,
kernel = "linear",
ranges = list(cost = 10^(seq(-5, -.5, length = 50))))
cv.wine.svc.linear = tune(svm,
as.numeric(quality) ~ .,
data = wine.train,
kernel = "linear",
ranges = list(cost = 10^(seq(-5, -.5, length = 50))))
View(wine)
cv.wine.svc.linear = tune(svm,
as.factor(quality) ~ .,
data = wine.train,
kernel = "linear",
ranges = list(cost = 10^(seq(-5, -.5, length = 50))))
cv.wine.svc.linear = tune(svm,
quality~ .,
data = wine.train,
kernel = "linear",
ranges = list(cost = 10^(seq(-5, -.5, length = 50))))
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message=FALSE)
library(HSAUR)
heptathlon
2. Visualize: Create a scatterplot matrix of all variables in the dataset. Notice anything?
```{r}
plot(heptathlon)
#It seems as though general performance is consistent among events; an athlete
#who performs particularly well in one event is likely to perform relatively
#well in another event.
#Alternate Solution
library(car)
n<-names(data.frame(heptathlon))
f<-as.formula(paste("~", paste(n[!n %in% "score"],collapse = "+")))
f
scatterplotMatrix(f,data=heptathlon)
heptathlon.new = heptathlon
heptathlon.new$hurdles = max(heptathlon$hurdles) - heptathlon$hurdles
heptathlon.new$run200m = max(heptathlon$run200m) - heptathlon$run200m
heptathlon.new$run800m = max(heptathlon$run800m) - heptathlon$run800m
View(heptathlon.new)
heptathlon.new
heptathlon.new
plot(heptathlon.new)
# Then we will examine the numerical values of the between pairs events correlations by
# applying the cor function
cor(heptathlon.new)
round(cor(heptathlon.new[,-8]), 2)
#--------------------------------use scatterplotMatrix----------------------------
n<-names(data.frame(heptathlon.new))
f<-as.formula(paste("~", paste(n[!n %in% "score"],collapse = "+")))
f
scatterplotMatrix(f,data=heptathlon.new)
library(psych)
fa.parallel(heptathlon.new[, -8], fa = "pc", n.iter = 100)
# show the eigen values for a principal components using fa = "pc"
abline(h = 1)
fa.parallel(heptathlon.new[, -8], fa = "pc", n.iter = 100)
# show the eigen values for a principal components using fa = "pc"
abline(h = 1)
fa.parallel(heptathlon.new[, -8], fa = "pc", n.iter = 100)
abline(h = 1)
fa.parallel(heptathlon.new[, -8], fa = "pc", n.iter = 100)
pc_heptathlon = principal(heptathlon.new[, -8], nfactors = 2, rotate = "none")
pc_heptathlon
factor.plot(pc_heptathlon, labels = colnames(heptathlon.new))
plot(pc_heptathlon$scores, type = "n") # type="n" does not produce any points or lines
text(pc_heptathlon$scores, rownames(heptathlon.new), cex = .75)
plot.new()
plot(pc_heptathlon$scores, type = "n") # type="n" does not produce any points or lines
text(pc_heptathlon$scores, rownames(heptathlon.new), cex = .75)
plot.new()
plot(pc_heptathlon$scores, type = "n") # type="n" does not produce any points or lines
text(pc_heptathlon$scores, rownames(heptathlon.new), cex = .75)
plot(pc_heptathlon$scores, type = "n") # type="n" does not produce any points or lines
plot.new()
text(pc_heptathlon$scores, rownames(heptathlon.new), cex = .75)
round(scale(heptathlon.new), 3)
round(heptathlon.new, 3)
setwd("C:/Users/Eugene/OneDrive/ML/ML_Project/eugene")
housing = read.csv("housing_LM.csv")
housing=as.data.frame(housing)
View(housing)
summary(housing)
sapply(housing, sd) #check std dev for df
cor(housing)
plot(housing)
#####################################################
#####Example using the State Information Dataset#####
#####################################################
help(state.x77)
state.x77 #Investigating the state.x77 dataset.
states = as.data.frame(state.x77) #Forcing the state.x77 dataset to be a dataframe.
#Cleaning up the column names so that there are no spaces.
colnames(states)[4] = "Life.Exp"
colnames(states)[6] = "HS.Grad"
#Creating a population density variable.
#feature engineering - make calc'd cols.
states[,9] = (states$Population*1000)/states$Area
colnames(states)[9] = "Density"
#Basic numerical EDA for states dataset.
summary(states)
sapply(states, sd) #check std dev for df
cor(states) #HS Grad and Illiteracy might be
#Basic graphical EDA for the states dataset.
plot(states)
model.saturated = lm(target ~ ., data = housing)
summary(model.saturated) #Many predictor variables are not significant, yet the
plot(model.saturated) #Assessing the assumptions of the model.
library(car)
influencePlot(model.saturated)
vif(model.saturated)
avPlots(model.saturated)
housing=housing[-c(1407),]
summary(housing)
model.saturated = lm(target ~ ., data = housing)
summary(model.saturated) #Many predictor variables are not significant, yet the
plot(model.saturated) #Assessing the assumptions of the model.
housing = read.csv("housing_LM.csv")
housing=as.data.frame(housing)
summary(housing)
sapply(housing, sd) #check std dev for df
cor(housing)
plot(housing)
model.saturated = lm(target ~ ., data = housing)
summary(model.saturated) #Many predictor variables are not significant, yet the
plot(model.saturated) #Assessing the assumptions of the model.
housing=housing[-c(1407,2367,154,2423,998),] #severe outlier
summary(housing)
sapply(housing, sd) #check std dev for df
cor(housing)
plot(housing)
model.saturated = lm(target ~ ., data = housing)
summary(model.saturated) #Many predictor variables are not significant, yet the
plot(model.saturated) #Assessing the assumptions of the model.
influencePlot(model.saturated)
vif(model.saturated)
avPlots(model.saturated)
#removing GarageArea due to high VIF and high P-vals
model2 = lm(Life.Exp ~ . - GarageCars, data = states)
#removing GarageArea due to high VIF and high P-vals
model2 = lm(Life.Exp ~ . -GarageCars, data = states)
#removing GarageArea due to high VIF and high P-vals
model2 = lm(Life.Exp ~ . -GarageCars, data = states)
#removing GarageArea due to high VIF and high P-vals
model2 = lm(Life.Exp ~ . - GarageCars, data = states)
View(states)
#removing GarageArea due to high VIF and high P-vals
model2 = lm(Life.Exp ~ . - GarageCars, data = housing)
#removing GarageArea due to high VIF and high P-vals
model2 = lm(target ~ . - GarageCars, data = housing)
#removing GarageArea due to high VIF and high P-vals
model2 = lm(target ~ . - GarageCars, data = housing)
anova(model2, model.saturated)
model.empty = lm(target ~ 1, data = housing) #The model with an intercept ONLY. (~1 *arg)
model.full = lm(target ~ ., data = housing)
scope = list(lower = formula(model.empty), upper = formula(model.full))
library(MASS) #The Modern Applied Statistics library.
forwardAIC = step(model.empty, scope, direction = "forward", k = 2)
summary(forwardAIC)
plot(forwardAIC)
influencePlot(forwardAIC)
vif(forwardAIC)
avPlots(forwardAIC)
confint(forwardAIC)
summary(forwardAIC)
forwardAIC$fitted.values
library(ggplot2)
ggplot()+geom_point(x=forwardAIC$fitted.values, y=housing$target)
ggplot()+geom_point(x=forwardAIC$fitted.values, y=housing$target)
ggplot()+geom_point(aes(x=forwardAIC$fitted.values, y=housing$target))
ggplot()+geom_point(aes(x=forwardAIC$fitted.values, y=housing$target))+geom_smooth()
ggplot(aes(x=forwardAIC$fitted.values, y=housing$target))+geom_point()+geom_smooth()
ggplot()+geom_point(aes(x=forwardAIC$fitted.values, y=housing$target))+geom_smooth()
ggplot()+geom_point(aes(x=forwardAIC$fitted.values, y=housing$target))+geom_smooth(aes(x=forwardAIC$fitted.values, y=housing$target))
ggplot()+geom_point(aes(x=forwardAIC$fitted.values, y=housing$target,col="#CC33FF", alpha=.5))+
geom_smooth(aes(x=forwardAIC$fitted.values, y=housing$target,col="#666666", se=FALSE))
ggplot()+geom_point(aes(x=forwardAIC$fitted.values, y=housing$target,col="#CC33FF", alpha=.5))+
geom_smooth(aes(x=forwardAIC$fitted.values, y=housing$target,col="#666666", se=FALSE))+
xlab("fitted values")+ ylab("actual values")
ggplot()+geom_point(aes(x=forwardAIC$fitted.values, y=housing$target,col="#CC33FF", alpha=.5))+
geom_smooth(aes(x=forwardAIC$fitted.values, y=housing$target,col="#666666", se=FALSE))+
xlab("fitted values")+ ylab("actual values")+ theme_bw()
ggplot()+geom_point(aes(x=forwardAIC$fitted.values, y=housing$target,col="#CC33FF", alpha=.5))+
geom_smooth(aes(x=forwardAIC$fitted.values, y=housing$target,col="#666666", se=FALSE))+
xlab("fitted values")+ ylab("actual values")+ theme_bw()+theme(legend.position = "None")
